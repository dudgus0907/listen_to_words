1
00:00:00,000 --> 00:00:03,120
In 5 years, we could have AGI systems

2
00:00:01,680 --> 00:00:04,880
that completely automate all human

3
00:00:03,120 --> 00:00:06,799
labor. AIs do all the repetitive work

4
00:00:04,880 --> 00:00:07,919
and humans do all the novel work. Every

5
00:00:06,799 --> 00:00:09,920
human is going to become a product

6
00:00:07,919 --> 00:00:11,200
manager of a team of AIs. How are you

7
00:00:09,920 --> 00:00:12,800
supposed to plan for that? I think it's

8
00:00:11,200 --> 00:00:14,719
extremely hard to predict where the

9
00:00:12,800 --> 00:00:16,720
world will be in 5 years. One-year plans

10
00:00:14,719 --> 00:00:18,240
make sense right now. Threeear plans are

11
00:00:16,720 --> 00:00:20,800
really hard and 5ear plans are

12
00:00:18,240 --> 00:00:22,480
impossible. So, because the AI market is

13
00:00:20,800 --> 00:00:24,080
changing so fast, like every month, like

14
00:00:22,480 --> 00:00:26,080
new systems come out that make new

15
00:00:24,080 --> 00:00:28,320
things possible. The right way of

16
00:00:26,080 --> 00:00:29,760
navigating that is to think from first

17
00:00:28,320 --> 00:00:31,279
principles about like what does the

18
00:00:29,760 --> 00:00:32,160
market need that still will be true in a

19
00:00:31,279 --> 00:00:33,680
year. Because if you're thinking about

20
00:00:32,160 --> 00:00:34,880
what does the market need right now, a

21
00:00:33,680 --> 00:00:36,320
month later something new is going to

22
00:00:34,880 --> 00:00:37,600
come out and they're not going to need

23
00:00:36,320 --> 00:00:38,719
it anymore. So you have to think a

24
00:00:37,600 --> 00:00:40,320
little more longterm. You have to be a

25
00:00:38,719 --> 00:00:41,600
little more strategic than in the past.

26
00:00:40,320 --> 00:00:43,600
You know, for example, like when I was

27
00:00:41,600 --> 00:00:45,280
in college, AI was not at all as

28
00:00:43,600 --> 00:00:46,960
prominent as it is today. Everyone today

29
00:00:45,280 --> 00:00:48,160
talks about AI, but there were a few

30
00:00:46,960 --> 00:00:50,239
people who were aware of what was going

31
00:00:48,160 --> 00:00:52,800
on. I was lucky enough to go to a

32
00:00:50,239 --> 00:00:54,800
Westworld watching party at OpenAI and I

33
00:00:52,800 --> 00:00:56,559
was actually on a bean bag with Greg

34
00:00:54,800 --> 00:00:58,079
Brockman um just chilling on this bean

35
00:00:56,559 --> 00:00:59,760
bag and we were arguing about the

36
00:00:58,079 --> 00:01:01,840
scaling hypothesis and the scaling

37
00:00:59,760 --> 00:01:04,239
hypothesis is this idea that you keep

38
00:01:01,840 --> 00:01:05,439
putting more compute into transformers

39
00:01:04,239 --> 00:01:07,000
and they'll just keep getting better and

40
00:01:05,439 --> 00:01:10,240
that's how we get to

41
00:01:07,000 --> 00:01:11,600
AGI. That was a crazy idea when I was in

42
00:01:10,240 --> 00:01:12,799
college. The vast majority of people

43
00:01:11,600 --> 00:01:14,720
didn't believe it. I didn't believe it.

44
00:01:12,799 --> 00:01:16,320
Greg was arguing that if we just keep

45
00:01:14,720 --> 00:01:18,479
scaling these things, we'll get there.

46
00:01:16,320 --> 00:01:20,479
We'll get to AGI. And I was arguing that

47
00:01:18,479 --> 00:01:22,240
we need new types of methods. I think we

48
00:01:20,479 --> 00:01:24,880
were both right in our own way, but we

49
00:01:22,240 --> 00:01:27,360
kept seeing progress from GB1 to GB2,

50
00:01:24,880 --> 00:01:29,520
GB3, eventually GB4. And at some point

51
00:01:27,360 --> 00:01:31,840
along that trajectory, I was like, "Holy

52
00:01:29,520 --> 00:01:34,000
cow, like these systems when you scale

53
00:01:31,840 --> 00:01:35,280
them, they just get really good." I

54
00:01:34,000 --> 00:01:37,520
think the people who were earlier to

55
00:01:35,280 --> 00:01:40,320
that conclusion did better. And we kind

56
00:01:37,520 --> 00:01:41,920
of apply the same scaling logic to Exa.

57
00:01:40,320 --> 00:01:44,159
We are building transformer-l like

58
00:01:41,920 --> 00:01:45,840
systems for search. And we also know

59
00:01:44,159 --> 00:01:47,600
that if you keep packing data and

60
00:01:45,840 --> 00:01:48,720
compute into the search engine, it will

61
00:01:47,600 --> 00:01:50,000
get better and better. That's a

62
00:01:48,720 --> 00:01:51,759
hypothesis we have. It's like our own

63
00:01:50,000 --> 00:01:52,640
scaling hypothesis for search. It's very

64
00:01:51,759 --> 00:01:54,240
different from traditional search

65
00:01:52,640 --> 00:01:56,240
engines like Google or Bing which

66
00:01:54,240 --> 00:01:58,880
basically have stayed the same for you

67
00:01:56,240 --> 00:02:00,479
know many years whereas like X is

68
00:01:58,880 --> 00:02:01,840
getting better like this. And so we're

69
00:02:00,479 --> 00:02:03,600
thinking a lot about like where is the

70
00:02:01,840 --> 00:02:05,520
future going? Uh so we see a world where

71
00:02:03,600 --> 00:02:07,840
there are agents everywhere. GBD5 level

72
00:02:05,520 --> 00:02:09,920
AI agents navigating the web doing all

73
00:02:07,840 --> 00:02:11,360
sorts of tasks. This future is coming.

74
00:02:09,920 --> 00:02:14,000
They're going to need search all of

75
00:02:11,360 --> 00:02:14,000
them.

76
00:02:14,160 --> 00:02:17,840
Hey, I'm Will. I'm the CEO of Exa. We're

77
00:02:16,319 --> 00:02:19,920
building the next generation of search.

78
00:02:17,840 --> 00:02:21,599
One good way of understanding Exa is in

79
00:02:19,920 --> 00:02:23,280
contrast to traditional search. So,

80
00:02:21,599 --> 00:02:24,879
traditional search engines use mostly

81
00:02:23,280 --> 00:02:26,640
keywords. Okay? So, if you're using a

82
00:02:24,879 --> 00:02:28,879
traditional search engine and you want

83
00:02:26,640 --> 00:02:31,280
to find startups working on futuristic

84
00:02:28,879 --> 00:02:32,480
hardware in the Bay Area, traditional

85
00:02:31,280 --> 00:02:34,160
search engines will use keyword

86
00:02:32,480 --> 00:02:35,760
matchings. The results they give you

87
00:02:34,160 --> 00:02:38,319
will be documents that contain the words

88
00:02:35,760 --> 00:02:39,680
startup and hardware and Bay Area. But

89
00:02:38,319 --> 00:02:40,879
startups that are working on futures of

90
00:02:39,680 --> 00:02:42,239
hardware in the Bay Area, they don't

91
00:02:40,879 --> 00:02:45,200
typically have those terms. Like you

92
00:02:42,239 --> 00:02:46,319
might have a a rocket company in SF. A

93
00:02:45,200 --> 00:02:48,400
traditional search engine won't be able

94
00:02:46,319 --> 00:02:50,480
to find that rocket company. But Exa can

95
00:02:48,400 --> 00:02:51,760
because uh we understand the meaning of

96
00:02:50,480 --> 00:02:53,680
documents. We understand the meaning

97
00:02:51,760 --> 00:02:55,760
that oh, if it's a rocket company in SF,

98
00:02:53,680 --> 00:02:58,080
then it does match startups working on

99
00:02:55,760 --> 00:02:59,280
futures of hardware in the Bay Area. We

100
00:02:58,080 --> 00:03:01,680
believe it's possible to have perfect

101
00:02:59,280 --> 00:03:03,440
search over the web, meaning whatever

102
00:03:01,680 --> 00:03:05,599
information you want, you get exactly

103
00:03:03,440 --> 00:03:07,040
that. We help companies integrate this

104
00:03:05,599 --> 00:03:09,040
high-quality knowledge into their

105
00:03:07,040 --> 00:03:11,280
applications. So we recently raised $17

106
00:03:09,040 --> 00:03:12,879
million from Lightseed and Nvidia. Our

107
00:03:11,280 --> 00:03:15,920
revenue is doubling every quarter. We're

108
00:03:12,879 --> 00:03:15,920
building the next generation of

109
00:03:17,720 --> 00:03:21,599
search. I came into college wanting to

110
00:03:20,159 --> 00:03:23,200
study physics to like I want to

111
00:03:21,599 --> 00:03:24,239
understand how the universe works and I

112
00:03:23,200 --> 00:03:25,920
thought physics was the right way to do

113
00:03:24,239 --> 00:03:27,760
that. Something big that influenced me

114
00:03:25,920 --> 00:03:29,200
was watching the social network cuz I I

115
00:03:27,760 --> 00:03:30,480
was studying at Harvard and the social

116
00:03:29,200 --> 00:03:32,000
network took place in Harvard. It was

117
00:03:30,480 --> 00:03:34,319
actually a very accurate movie and it

118
00:03:32,000 --> 00:03:36,720
was very inspiring to see this guy

119
00:03:34,319 --> 00:03:37,760
change the world just on his laptop. I

120
00:03:36,720 --> 00:03:39,599
realized that you could have like a

121
00:03:37,760 --> 00:03:41,599
massive influence just coding on your

122
00:03:39,599 --> 00:03:43,120
laptop in a way that you couldn't as

123
00:03:41,599 --> 00:03:45,200
much with physics. It was very clear

124
00:03:43,120 --> 00:03:47,040
that that that the AI could understand

125
00:03:45,200 --> 00:03:48,720
the problems of physics. So I went into

126
00:03:47,040 --> 00:03:50,879
computer science and I think that turned

127
00:03:48,720 --> 00:03:52,319
out to be right because now like the AI

128
00:03:50,879 --> 00:03:54,000
is getting so good that it should be

129
00:03:52,319 --> 00:03:57,040
able to just tell us the answer or

130
00:03:54,000 --> 00:03:59,200
infuse it into our brains. Before EXA on

131
00:03:57,040 --> 00:04:00,799
the side I was writing a history book. I

132
00:03:59,200 --> 00:04:02,640
got really excited about world history

133
00:04:00,799 --> 00:04:04,879
and I decided I'm just gonna write a

134
00:04:02,640 --> 00:04:07,280
book about it because I'm like no one

135
00:04:04,879 --> 00:04:08,799
has captured in a book my the level of

136
00:04:07,280 --> 00:04:10,560
excitement that I had and so I was doing

137
00:04:08,799 --> 00:04:12,480
a lot of research for the book. I

138
00:04:10,560 --> 00:04:13,760
quickly realized that Google is actually

139
00:04:12,480 --> 00:04:15,040
not good enough for that type of

140
00:04:13,760 --> 00:04:17,440
research. Like Google is great for

141
00:04:15,040 --> 00:04:19,199
surface level investigations. Once you

142
00:04:17,440 --> 00:04:21,440
start like trying to go deeper and

143
00:04:19,199 --> 00:04:23,280
trying to understand any topic deeply on

144
00:04:21,440 --> 00:04:24,960
the web, Google really fails. For

145
00:04:23,280 --> 00:04:27,360
example, if I want to find like all the

146
00:04:24,960 --> 00:04:28,720
research papers on uh poverty in ancient

147
00:04:27,360 --> 00:04:30,400
Rome, it's actually really hard to find

148
00:04:28,720 --> 00:04:31,919
that on Google. Not every paper will

149
00:04:30,400 --> 00:04:33,919
mention the word poverty. And so I was

150
00:04:31,919 --> 00:04:35,199
doing the research for this book and it

151
00:04:33,919 --> 00:04:37,040
was really hard to find things. And then

152
00:04:35,199 --> 00:04:39,759
at the same time, GB3 had recently come

153
00:04:37,040 --> 00:04:42,320
out and GB3 was this magical creature

154
00:04:39,759 --> 00:04:44,960
really that I could talk to and it could

155
00:04:42,320 --> 00:04:46,800
understand like anything I say at a very

156
00:04:44,960 --> 00:04:49,199
deep complex level. And so the thinking

157
00:04:46,800 --> 00:04:51,680
was what if we could apply the same

158
00:04:49,199 --> 00:04:53,280
technology of GB3 to search? What if you

159
00:04:51,680 --> 00:04:55,040
can make a search engine that actually

160
00:04:53,280 --> 00:04:58,320
understands you at a deep level? And

161
00:04:55,040 --> 00:04:58,320
it's been the same goal ever

162
00:05:00,280 --> 00:05:06,400
since. The first year and a half of EXO,

163
00:05:03,280 --> 00:05:08,400
we did research into uh search models

164
00:05:06,400 --> 00:05:10,639
into how can we take transformer models

165
00:05:08,400 --> 00:05:12,639
and apply them to a search engine. No

166
00:05:10,639 --> 00:05:14,320
one's really done that before. It took a

167
00:05:12,639 --> 00:05:16,080
long time to figure out how to do it

168
00:05:14,320 --> 00:05:17,280
well. And that required persistence.

169
00:05:16,080 --> 00:05:18,880
like we were just banging our head

170
00:05:17,280 --> 00:05:20,320
against the wall for a year and a half

171
00:05:18,880 --> 00:05:22,160
trying out different models, trying out

172
00:05:20,320 --> 00:05:23,360
different data sets and eventually we

173
00:05:22,160 --> 00:05:24,720
got something that worked really well.

174
00:05:23,360 --> 00:05:25,919
If we didn't have the persistence, you

175
00:05:24,720 --> 00:05:28,000
know, 6 months in, we might have given

176
00:05:25,919 --> 00:05:30,080
up, but we didn't. Early November 2022,

177
00:05:28,000 --> 00:05:32,000
we launched the first version of Exa to

178
00:05:30,080 --> 00:05:33,759
the public. We built a search engine

179
00:05:32,000 --> 00:05:35,440
that was perfect for AI applications.

180
00:05:33,759 --> 00:05:36,720
Basically, AI systems, they have all

181
00:05:35,440 --> 00:05:38,479
this intelligence, but they're lacking

182
00:05:36,720 --> 00:05:40,479
in knowledge. And so, when they need

183
00:05:38,479 --> 00:05:42,160
knowledge, they go make a call to Exa

184
00:05:40,479 --> 00:05:45,039
and get exactly that knowledge. Then

185
00:05:42,160 --> 00:05:47,039
catch came out a few weeks later and

186
00:05:45,039 --> 00:05:48,880
that was a big moment uh for the

187
00:05:47,039 --> 00:05:49,919
information ecosystem. But for us it was

188
00:05:48,880 --> 00:05:52,000
really interesting because we started

189
00:05:49,919 --> 00:05:53,360
getting requests for API access to our

190
00:05:52,000 --> 00:05:55,759
search engine. We started getting

191
00:05:53,360 --> 00:05:57,759
requests for API access uh first from a

192
00:05:55,759 --> 00:05:59,919
friend who was actually like living

193
00:05:57,759 --> 00:06:01,440
downstairs. I told him no sorry we don't

194
00:05:59,919 --> 00:06:02,720
have API access. And I didn't really

195
00:06:01,440 --> 00:06:05,280
think much of it. Uh but then we kept

196
00:06:02,720 --> 00:06:06,960
then we got uh a request for API access

197
00:06:05,280 --> 00:06:08,720
from someone from some company in

198
00:06:06,960 --> 00:06:10,240
Germany. And we also told them no sorry

199
00:06:08,720 --> 00:06:12,479
we don't have an API. And then we kept

200
00:06:10,240 --> 00:06:15,039
getting requests for API access. And we

201
00:06:12,479 --> 00:06:17,199
realized that because of chatbt, people

202
00:06:15,039 --> 00:06:19,039
were starting to build AI applications

203
00:06:17,199 --> 00:06:20,400
all over the place for all sorts of

204
00:06:19,039 --> 00:06:22,639
businesses. And all these AI

205
00:06:20,400 --> 00:06:24,080
applications needed search. Like the AIS

206
00:06:22,639 --> 00:06:26,319
themselves needed to search and that's

207
00:06:24,080 --> 00:06:28,080
when we started to realize, okay, X

208
00:06:26,319 --> 00:06:30,160
could be really useful for these AI

209
00:06:28,080 --> 00:06:32,479
applications. Yeah. So our initial

210
00:06:30,160 --> 00:06:34,639
customer found us. And so one lesson

211
00:06:32,479 --> 00:06:35,919
there is just like be a really good

212
00:06:34,639 --> 00:06:37,680
listener to the market. Like what are

213
00:06:35,919 --> 00:06:40,080
people repeatedly saying they need? and

214
00:06:37,680 --> 00:06:41,759
you might have some idea of what you

215
00:06:40,080 --> 00:06:43,360
know what you're going to sell but then

216
00:06:41,759 --> 00:06:44,800
if people keep requesting something like

217
00:06:43,360 --> 00:06:46,240
API access maybe you should start

218
00:06:44,800 --> 00:06:48,720
listening to them like we cared more

219
00:06:46,240 --> 00:06:50,960
about uh learnings from the customer

220
00:06:48,720 --> 00:06:52,720
than getting a lot of revenue and so

221
00:06:50,960 --> 00:06:54,639
yeah we were like opening our ears to

222
00:06:52,720 --> 00:06:57,120
what do the customers need uh over time

223
00:06:54,639 --> 00:06:59,039
developed a hypothesis about how we

224
00:06:57,120 --> 00:07:01,520
should price and what types of customers

225
00:06:59,039 --> 00:07:01,520
we should

226
00:07:03,400 --> 00:07:08,960
pursue. I think you can guess where

227
00:07:06,400 --> 00:07:11,280
companies like OpenAI and Enthropic are

228
00:07:08,960 --> 00:07:12,639
going to build based on like what are

229
00:07:11,280 --> 00:07:14,720
like what are big markets that they

230
00:07:12,639 --> 00:07:17,199
could tackle like agents like automating

231
00:07:14,720 --> 00:07:18,720
work is a clear huge market and so

232
00:07:17,199 --> 00:07:20,240
they're clearly going to do that. Okay.

233
00:07:18,720 --> 00:07:22,160
So now you know what types of things

234
00:07:20,240 --> 00:07:24,319
they want to do. Can they do it? Well

235
00:07:22,160 --> 00:07:27,360
then you think about fundamentals of

236
00:07:24,319 --> 00:07:28,720
LLMs. You think like okay LLM can as

237
00:07:27,360 --> 00:07:30,560
long as you can make training data for

238
00:07:28,720 --> 00:07:32,080
some objective they will get better at

239
00:07:30,560 --> 00:07:34,160
that objective. Can you make training

240
00:07:32,080 --> 00:07:37,599
data for agentic behavior? Definitely

241
00:07:34,160 --> 00:07:39,280
you just have a bunch of examples of

242
00:07:37,599 --> 00:07:41,280
navigating the web in order to buy plane

243
00:07:39,280 --> 00:07:43,199
tickets and if you have a million

244
00:07:41,280 --> 00:07:45,199
examples of that then now the LLM will

245
00:07:43,199 --> 00:07:47,440
know how to buy plane tickets. Yes. So

246
00:07:45,199 --> 00:07:50,240
any sort of task such that you could

247
00:07:47,440 --> 00:07:51,840
create data for it LLM will get near

248
00:07:50,240 --> 00:07:53,120
perfect at that task. So that's like a

249
00:07:51,840 --> 00:07:55,520
fundamental way of thinking about this.

250
00:07:53,120 --> 00:07:57,680
So will AI get really good at navigating

251
00:07:55,520 --> 00:07:57,680
the

252
00:07:58,120 --> 00:08:02,240
web? Yes. That's pretty easy because

253
00:08:00,639 --> 00:08:04,080
it's it's easy to generate lots of

254
00:08:02,240 --> 00:08:06,560
navigating the web data. Will AI get

255
00:08:04,080 --> 00:08:06,560
really good at

256
00:08:07,160 --> 00:08:12,000
robotics? Yes, but probably on a longer

257
00:08:09,440 --> 00:08:14,400
term horizon because gathering lots of

258
00:08:12,000 --> 00:08:16,080
robotic training data is harder because

259
00:08:14,400 --> 00:08:17,280
it's hardware and hardware is hard to

260
00:08:16,080 --> 00:08:18,879
work with. So what do we learn from

261
00:08:17,280 --> 00:08:21,120
that? We learned that AI's navigating

262
00:08:18,879 --> 00:08:23,199
the web will come pretty soon. AI's

263
00:08:21,120 --> 00:08:25,199
navigating robotics will take longer. So

264
00:08:23,199 --> 00:08:26,400
just from like simple first principles,

265
00:08:25,199 --> 00:08:28,960
you can like guess where things are

266
00:08:26,400 --> 00:08:30,800
going. Picture in your mind how crazy

267
00:08:28,960 --> 00:08:35,399
good technology will be and AI will be

268
00:08:30,800 --> 00:08:35,399
in a year and then build for that world.

