1
00:00:00,160 --> 00:00:03,760
Hello, my name is Roy. I founded this

2
00:00:02,080 --> 00:00:05,440
company called Cluey and I just got

3
00:00:03,760 --> 00:00:07,200
kicked out of Columbia for uh building

4
00:00:05,440 --> 00:00:09,040
this tool called interview coder. It's

5
00:00:07,200 --> 00:00:10,800
like a cheating tool for software

6
00:00:09,040 --> 00:00:12,519
engineering technical interviews. I use

7
00:00:10,800 --> 00:00:15,040
the technology to build a much bigger

8
00:00:12,519 --> 00:00:17,039
company, the desktop app that lets you

9
00:00:15,040 --> 00:00:19,279
cheat on everything. Right now, we just

10
00:00:17,039 --> 00:00:21,359
launched about a month ago, closing in

11
00:00:19,279 --> 00:00:23,920
on $5 million in annually recurring

12
00:00:21,359 --> 00:00:26,480
revenue. And we also just closed a $5.3

13
00:00:23,920 --> 00:00:30,599
million seed round led by Abstract

14
00:00:26,480 --> 00:00:30,599
Ventures and SUSA Ventures.

15
00:00:32,399 --> 00:00:36,160
I I was a pretty I was pretty wild kid.

16
00:00:34,239 --> 00:00:37,360
I got in a lot of trouble. I was pretty

17
00:00:36,160 --> 00:00:39,040
smart. I think I was pretty good at

18
00:00:37,360 --> 00:00:40,719
math. I did a bit of math competition

19
00:00:39,040 --> 00:00:43,200
when I was younger. I was on like the

20
00:00:40,719 --> 00:00:44,719
debate team. I played some cello. I love

21
00:00:43,200 --> 00:00:46,719
girls, you know. I every year I had a

22
00:00:44,719 --> 00:00:48,879
new girlfriend. Yeah. My mom made me do

23
00:00:46,719 --> 00:00:50,079
a lot of studying, but I hated studying.

24
00:00:48,879 --> 00:00:51,680
I was always trying to go out and like

25
00:00:50,079 --> 00:00:53,280
like like like like have fun, play with

26
00:00:51,680 --> 00:00:54,960
my friends. I've always been very

27
00:00:53,280 --> 00:00:56,239
competitive when I was a kid. I had a I

28
00:00:54,960 --> 00:00:58,079
have an older brother who was 2 years

29
00:00:56,239 --> 00:00:59,359
older than me. I always wanted to be

30
00:00:58,079 --> 00:01:00,800
like smarter than him. I always wanted

31
00:00:59,359 --> 00:01:02,480
to do better in great at school than

32
00:01:00,800 --> 00:01:04,400
him. I I'm always trying to win. I'm

33
00:01:02,480 --> 00:01:06,000
trying to be chongu at everything, you

34
00:01:04,400 --> 00:01:07,439
know, like top of the class. In the

35
00:01:06,000 --> 00:01:09,119
world of software engineering, if you

36
00:01:07,439 --> 00:01:10,479
want to get a job at a big tech company,

37
00:01:09,119 --> 00:01:12,799
you have to answer these sort of

38
00:01:10,479 --> 00:01:14,799
riddlelesque questions that are called

39
00:01:12,799 --> 00:01:16,400
code questions. And pretty much every

40
00:01:14,799 --> 00:01:18,560
developer you know at a big company has

41
00:01:16,400 --> 00:01:20,720
gone through the gauntlet of memorizing

42
00:01:18,560 --> 00:01:22,080
600 300 riddles and just sort of like

43
00:01:20,720 --> 00:01:23,840
memorizing the solutions and

44
00:01:22,080 --> 00:01:26,080
regurgitating them in interviews. I'm

45
00:01:23,840 --> 00:01:28,000
very very competitive. So, the second I

46
00:01:26,080 --> 00:01:29,600
knew that there was like a a ranking on

47
00:01:28,000 --> 00:01:31,280
League Code, like a global ranking, I

48
00:01:29,600 --> 00:01:33,119
knew I had to be like one of the best.

49
00:01:31,280 --> 00:01:34,479
So, I uh I spent hundreds of hours

50
00:01:33,119 --> 00:01:35,840
studying Grinding the Riddles, even

51
00:01:34,479 --> 00:01:37,360
though I like I don't care about League

52
00:01:35,840 --> 00:01:38,560
Code. I didn't enjoy it. I didn't really

53
00:01:37,360 --> 00:01:39,920
have a good time, but I was just

54
00:01:38,560 --> 00:01:41,360
competitive. So, I thought like there's

55
00:01:39,920 --> 00:01:42,880
a ranking, then I got to be on the top

56
00:01:41,360 --> 00:01:44,479
of the ranking. But, I mean, it just

57
00:01:42,880 --> 00:01:46,399
ended up with me wasting a bunch of

58
00:01:44,479 --> 00:01:48,240
hours. Le code just has nothing to do

59
00:01:46,399 --> 00:01:50,079
with what you do on the job. It's like

60
00:01:48,240 --> 00:01:51,920
the modern-day equivalent of asking how

61
00:01:50,079 --> 00:01:53,280
many balloons fit in the Empire State

62
00:01:51,920 --> 00:01:55,280
Building. It's supposed to test your

63
00:01:53,280 --> 00:01:57,439
critical thinking, but the questions are

64
00:01:55,280 --> 00:01:58,960
online to the extent that rather than

65
00:01:57,439 --> 00:02:00,479
practicing critical thinking, you just

66
00:01:58,960 --> 00:02:01,680
practice memorizing the riddles. You're

67
00:02:00,479 --> 00:02:03,200
going to sit through and memorize all

68
00:02:01,680 --> 00:02:05,520
the 10,000 questions cuz it means you

69
00:02:03,200 --> 00:02:07,280
get a 200k a year job. This is not good

70
00:02:05,520 --> 00:02:08,800
for anybody. You don't you don't learn

71
00:02:07,280 --> 00:02:10,239
anything from practicing these riddles

72
00:02:08,800 --> 00:02:11,599
and you just end up wasting time when

73
00:02:10,239 --> 00:02:13,120
you should be programming. I thought

74
00:02:11,599 --> 00:02:14,800
this was pretty stupid and this has been

75
00:02:13,120 --> 00:02:16,239
going on for around 20 years. Right now,

76
00:02:14,800 --> 00:02:18,239
the technology was there to sort of

77
00:02:16,239 --> 00:02:19,920
develop this tool that would invisibly

78
00:02:18,239 --> 00:02:21,760
let you use AI to cheat on these

79
00:02:19,920 --> 00:02:23,520
interviews. So, I built the tool. I

80
00:02:21,760 --> 00:02:25,840
publicly recorded myself using it on the

81
00:02:23,520 --> 00:02:27,440
Amazon interview. I got the job and I I

82
00:02:25,840 --> 00:02:29,280
posted this everywhere saying, "Look how

83
00:02:27,440 --> 00:02:30,879
easy it is to hack these interviews."

84
00:02:29,280 --> 00:02:33,040
Eventually, this this this got me in

85
00:02:30,879 --> 00:02:35,200
some trouble. But, uh, the impetus of

86
00:02:33,040 --> 00:02:36,959
everything was when I decided that it's

87
00:02:35,200 --> 00:02:38,360
just a stupid industry practice and I

88
00:02:36,959 --> 00:02:40,480
wanted to change

89
00:02:38,360 --> 00:02:42,640
it. A lot of people think it was

90
00:02:40,480 --> 00:02:44,160
explosive from the start, but it it took

91
00:02:42,640 --> 00:02:46,319
about a month before it really started

92
00:02:44,160 --> 00:02:48,319
to take off. And for a month I posted

93
00:02:46,319 --> 00:02:50,080
everything Amazon saw was getting mad at

94
00:02:48,319 --> 00:02:52,080
me and Colombia saw and was getting mad

95
00:02:50,080 --> 00:02:53,519
at me and everyone was just upset and it

96
00:02:52,080 --> 00:02:54,879
didn't really go that viral for about a

97
00:02:53,519 --> 00:02:56,720
month. And during that time I was really

98
00:02:54,879 --> 00:02:58,800
stressed. I just gave up my entire

99
00:02:56,720 --> 00:03:00,480
career and my entire education for the

100
00:02:58,800 --> 00:03:01,920
hope of a company. But it didn't even go

101
00:03:00,480 --> 00:03:03,920
viral. Like I did all this for like

102
00:03:01,920 --> 00:03:05,440
15,000 views. And I was really worried.

103
00:03:03,920 --> 00:03:06,720
Everybody in my life including even my

104
00:03:05,440 --> 00:03:07,599
co-founders were telling me like, "Hey,

105
00:03:06,720 --> 00:03:09,040
we should probably stop. We should

106
00:03:07,599 --> 00:03:10,319
probably shut this down." But I don't

107
00:03:09,040 --> 00:03:12,000
know. There was just a voice in my head

108
00:03:10,319 --> 00:03:13,280
that said like this is something this

109
00:03:12,000 --> 00:03:14,959
has potential. Like I have to keep

110
00:03:13,280 --> 00:03:17,280
going. And I did keep going. And then at

111
00:03:14,959 --> 00:03:19,920
at one point it did go viral, like super

112
00:03:17,280 --> 00:03:22,080
viral, and everybody on tech saw it. At

113
00:03:19,920 --> 00:03:24,080
that point, I was safe. Virality

114
00:03:22,080 --> 00:03:25,840
protected me from further punishment

115
00:03:24,080 --> 00:03:27,360
from Colombia. It made the path to

116
00:03:25,840 --> 00:03:29,200
entrepreneurship a lot easier and

117
00:03:27,360 --> 00:03:31,120
clearer. I've sort of like committed my

118
00:03:29,200 --> 00:03:32,640
life to building companies. Once I made

119
00:03:31,120 --> 00:03:34,480
that decision, it was it was very easy

120
00:03:32,640 --> 00:03:36,400
for me to decide to leave Colombia. And

121
00:03:34,480 --> 00:03:37,840
once I did leave Colombia, like I knew

122
00:03:36,400 --> 00:03:39,440
I'm going to do the only thing that I

123
00:03:37,840 --> 00:03:41,280
can do now, which is build companies.

124
00:03:39,440 --> 00:03:43,040
Uh, I think it also helps position me

125
00:03:41,280 --> 00:03:44,720
like I I'm a I'm a very unique person

126
00:03:43,040 --> 00:03:46,319
now who got kicked out of an Ivy League

127
00:03:44,720 --> 00:03:48,319
and as a result there there's a lot of

128
00:03:46,319 --> 00:03:50,080
interest from from Silicon Valley about

129
00:03:48,319 --> 00:03:51,519
what I'm going to do next. To me at the

130
00:03:50,080 --> 00:03:53,519
time it felt like things were moving

131
00:03:51,519 --> 00:03:55,440
really slow. As soon as interview coder

132
00:03:53,519 --> 00:03:56,879
went viral I knew that I had to

133
00:03:55,440 --> 00:03:58,239
capitalize on the moment cuz the

134
00:03:56,879 --> 00:04:00,400
attention wasn't going to be there for

135
00:03:58,239 --> 00:04:02,080
long. The product was a product designed

136
00:04:00,400 --> 00:04:03,439
to die. It's a product to cheat on

137
00:04:02,080 --> 00:04:04,959
technical interviews. So the second

138
00:04:03,439 --> 00:04:07,040
companies change technical interviews,

139
00:04:04,959 --> 00:04:09,760
the product dies. meaning I have a few

140
00:04:07,040 --> 00:04:11,519
short moments before uh my my spot in

141
00:04:09,760 --> 00:04:12,959
the limelight dies and I thought I have

142
00:04:11,519 --> 00:04:14,640
to raise around. I have to start a

143
00:04:12,959 --> 00:04:17,199
bigger company that's more sustainable

144
00:04:14,640 --> 00:04:19,040
and defensible longterm. So it for me it

145
00:04:17,199 --> 00:04:20,799
felt like things were moving so slow. I

146
00:04:19,040 --> 00:04:22,400
had to push back my fund raise like 2

147
00:04:20,799 --> 00:04:24,479
weeks. I was going to fund raise 2 weeks

148
00:04:22,400 --> 00:04:26,400
earlier than I did. To me at the time I

149
00:04:24,479 --> 00:04:28,479
felt like I was in such a time crunch

150
00:04:26,400 --> 00:04:30,080
and I and I had I had to uh I had to

151
00:04:28,479 --> 00:04:31,360
wrap things up and and do the next thing

152
00:04:30,080 --> 00:04:33,120
and it felt like things were moving

153
00:04:31,360 --> 00:04:36,160
super slow. But uh I guess I I was under

154
00:04:33,120 --> 00:04:38,240
the pressure of the situation.

155
00:04:36,160 --> 00:04:40,000
Yeah, I mean we planned for me to use

156
00:04:38,240 --> 00:04:41,600
interview coder to like cheat on a bunch

157
00:04:40,000 --> 00:04:42,960
of big tech interviews and and get the

158
00:04:41,600 --> 00:04:44,880
jobs and we thought that was going to be

159
00:04:42,960 --> 00:04:46,720
a viral moment. So in that sense we

160
00:04:44,880 --> 00:04:48,160
planned it, but the last 3 months I've

161
00:04:46,720 --> 00:04:50,080
made probably like maybe like a thousand

162
00:04:48,160 --> 00:04:51,840
tweets and since then like I figured out

163
00:04:50,080 --> 00:04:53,199
how to make tweets that will go viral,

164
00:04:51,840 --> 00:04:54,800
how to make tweets that will be more

165
00:04:53,199 --> 00:04:57,199
controversial and and and get more

166
00:04:54,800 --> 00:04:59,280
engagement. I think for X especially,

167
00:04:57,199 --> 00:05:00,639
well I've only really cracked X. Uh I

168
00:04:59,280 --> 00:05:02,800
think because I think people on Twitter

169
00:05:00,639 --> 00:05:04,639
are a very unique type of people. Um,

170
00:05:02,800 --> 00:05:06,240
they love controversy, they love drama,

171
00:05:04,639 --> 00:05:08,639
they love attention, and they love to

172
00:05:06,240 --> 00:05:10,400
either dog on people or watch people get

173
00:05:08,639 --> 00:05:12,320
dogged on. I think every single time you

174
00:05:10,400 --> 00:05:14,320
tweet something, if you don't think half

175
00:05:12,320 --> 00:05:16,240
the people in the world would feel very

176
00:05:14,320 --> 00:05:18,080
negative about this, then it's probably

177
00:05:16,240 --> 00:05:19,840
not going to be viral as a tweet. All of

178
00:05:18,080 --> 00:05:21,600
your tweets that will are you're

179
00:05:19,840 --> 00:05:23,199
planning on making go viral, they need

180
00:05:21,600 --> 00:05:24,800
to have a very strong controversial

181
00:05:23,199 --> 00:05:26,160
twist that makes people pause and be

182
00:05:24,800 --> 00:05:28,240
like, "What the fuck?" And this is not

183
00:05:26,160 --> 00:05:30,080
the case for Instagram, Tik Tok, or

184
00:05:28,240 --> 00:05:31,600
LinkedIn or whatever, but it is the case

185
00:05:30,080 --> 00:05:32,960
for Twitter. And yeah, I think I think

186
00:05:31,600 --> 00:05:34,479
Twitter Next, the more controversial

187
00:05:32,960 --> 00:05:36,000
you're willing to make your tweet, the

188
00:05:34,479 --> 00:05:38,160
better and the more viral you will go.

189
00:05:36,000 --> 00:05:40,240
Interview coder on Twitter was received

190
00:05:38,160 --> 00:05:42,560
so positively. It was just like this

191
00:05:40,240 --> 00:05:44,400
scrappy young kid who was trying to

192
00:05:42,560 --> 00:05:46,479
fight back against big industry, big

193
00:05:44,400 --> 00:05:48,800
tech. And uh everyone on Twitter was

194
00:05:46,479 --> 00:05:50,639
very supportive. I think as I got bigger

195
00:05:48,800 --> 00:05:52,080
and my account grew, people grew less

196
00:05:50,639 --> 00:05:54,080
supportive, which is like to be

197
00:05:52,080 --> 00:05:56,320
expected. I'm generally very good at

198
00:05:54,080 --> 00:05:58,320
receiving hate and criticism. I' I'm a

199
00:05:56,320 --> 00:05:59,919
very polarizing personality and I do a

200
00:05:58,320 --> 00:06:01,280
lot of crazy stuff. Throughout my life,

201
00:05:59,919 --> 00:06:02,720
I've always had people like giving me

202
00:06:01,280 --> 00:06:03,919
hate. None of the negative comments

203
00:06:02,720 --> 00:06:05,280
really stood out, but I was very

204
00:06:03,919 --> 00:06:06,639
surprised to see how positively

205
00:06:05,280 --> 00:06:08,160
Interview Coder was received on Twitter

206
00:06:06,639 --> 00:06:09,919
when I first launched it. I think people

207
00:06:08,160 --> 00:06:11,600
are often so worried that they're going

208
00:06:09,919 --> 00:06:13,039
to say something bad online and it's

209
00:06:11,600 --> 00:06:15,039
just going to get back to them and their

210
00:06:13,039 --> 00:06:16,560
their their reputation is over. And I

211
00:06:15,039 --> 00:06:18,479
don't know, like it's it's going to bury

212
00:06:16,560 --> 00:06:21,280
them. But I think in reality, all press

213
00:06:18,479 --> 00:06:23,280
is good press. I say a ton of super

214
00:06:21,280 --> 00:06:24,800
controversial stuff. And in every video

215
00:06:23,280 --> 00:06:26,560
I'm in, there's like a bunch of comments

216
00:06:24,800 --> 00:06:27,759
saying, "Oh, this guy's evil. This guy's

217
00:06:26,560 --> 00:06:28,960
like like this, this, and that." And

218
00:06:27,759 --> 00:06:30,479
there's always some hate, but it's like

219
00:06:28,960 --> 00:06:32,000
like this stuff really doesn't matter.

220
00:06:30,479 --> 00:06:33,680
Yeah. I mean, like I I've learned that

221
00:06:32,000 --> 00:06:35,039
it really doesn't matter if if if

222
00:06:33,680 --> 00:06:36,400
everybody in the world just doesn't like

223
00:06:35,039 --> 00:06:37,840
you. Well, actually, it's it's done

224
00:06:36,400 --> 00:06:39,919
pretty much the opposite. I've realized

225
00:06:37,840 --> 00:06:41,919
that even if I say extremely crazy

226
00:06:39,919 --> 00:06:43,680
online, it will just make people more

227
00:06:41,919 --> 00:06:45,199
interested in me and the company, and

228
00:06:43,680 --> 00:06:46,720
it'll just drive more downloads and

229
00:06:45,199 --> 00:06:48,880
conversions and get more eyeballs onto

230
00:06:46,720 --> 00:06:50,880
Clue. If anything, I've learned that I

231
00:06:48,880 --> 00:06:53,280
need to become crazier online so that

232
00:06:50,880 --> 00:06:55,600
people will keep funneling attention

233
00:06:53,280 --> 00:06:57,440
towards the core product. I think uh

234
00:06:55,600 --> 00:06:59,120
there's it's very very rare that you

235
00:06:57,440 --> 00:07:00,479
will say something online that

236
00:06:59,120 --> 00:07:02,400
translates to something negative

237
00:07:00,479 --> 00:07:04,240
happening in the real world. Like online

238
00:07:02,400 --> 00:07:06,240
is not real life. I'm a pretty chill

239
00:07:04,240 --> 00:07:08,240
person in real life, but online I'm

240
00:07:06,240 --> 00:07:10,400
crazy because uh it gets it gets me

241
00:07:08,240 --> 00:07:13,440
engagement and attention. When you are

242
00:07:10,400 --> 00:07:15,680
so polarizing and controversial online,

243
00:07:13,440 --> 00:07:17,759
you need to very clearly distinguish

244
00:07:15,680 --> 00:07:20,000
this is my real world life and this is

245
00:07:17,759 --> 00:07:22,560
the online life. And in my real world

246
00:07:20,000 --> 00:07:24,960
life, there are very few people who I I

247
00:07:22,560 --> 00:07:26,960
trust fully and who I think I love and

248
00:07:24,960 --> 00:07:28,479
love me. My parents will always be on my

249
00:07:26,960 --> 00:07:30,639
side no matter what crazy I do

250
00:07:28,479 --> 00:07:31,919
online. And my wife and kids, they will

251
00:07:30,639 --> 00:07:33,520
always be on my side no matter what

252
00:07:31,919 --> 00:07:35,759
crazy I do online. And I think it

253
00:07:33,520 --> 00:07:37,759
is very very important to distinguish

254
00:07:35,759 --> 00:07:39,280
this is my real life uh family and

255
00:07:37,759 --> 00:07:41,440
friends and they love me unconditionally

256
00:07:39,280 --> 00:07:43,199
and I love them and everything online is

257
00:07:41,440 --> 00:07:45,520
just noise. Even if everyone online or

258
00:07:43,199 --> 00:07:46,800
everyone outside this box hates me, it

259
00:07:45,520 --> 00:07:48,400
doesn't matter because uh the most

260
00:07:46,800 --> 00:07:50,919
important people are in this box and

261
00:07:48,400 --> 00:07:54,240
they're people that I love and love me

262
00:07:50,919 --> 00:07:55,840
back. Interview coder is a tool designed

263
00:07:54,240 --> 00:07:57,759
to let you treat on technical

264
00:07:55,840 --> 00:08:00,000
interviews. But what we realized as we

265
00:07:57,759 --> 00:08:02,319
built interview coder is that the idea

266
00:08:00,000 --> 00:08:04,639
of an AI that sees your screen and hears

267
00:08:02,319 --> 00:08:06,319
your audio sort of shows itself as this

268
00:08:04,639 --> 00:08:08,080
translucent screen overlay. This has

269
00:08:06,319 --> 00:08:09,759
never really been attempted before. This

270
00:08:08,080 --> 00:08:11,440
is a completely novel user experience

271
00:08:09,759 --> 00:08:12,800
and it's very shortsighted to think that

272
00:08:11,440 --> 00:08:14,879
this is only good for cheating.

273
00:08:12,800 --> 00:08:16,479
Ultimately, what we're building is we're

274
00:08:14,879 --> 00:08:18,160
building for a future where models are

275
00:08:16,479 --> 00:08:19,280
multimodal and the models are not there

276
00:08:18,160 --> 00:08:21,440
yet and they're probably not going to be

277
00:08:19,280 --> 00:08:23,599
there yet for another 3 years. Nobody's

278
00:08:21,440 --> 00:08:25,120
really thought of what happens when chat

279
00:08:23,599 --> 00:08:26,560
bots are no longer relevant. What

280
00:08:25,120 --> 00:08:28,879
happens when you don't want to prompt

281
00:08:26,560 --> 00:08:30,800
GPT anymore and AI just knows what you

282
00:08:28,879 --> 00:08:32,560
want? Then how will you interact with

283
00:08:30,800 --> 00:08:34,399
AI? Nobody's really attempted this

284
00:08:32,560 --> 00:08:36,880
before. And I think um interview coder

285
00:08:34,399 --> 00:08:38,240
was the first proof of concept of a user

286
00:08:36,880 --> 00:08:40,399
experience that could work in this

287
00:08:38,240 --> 00:08:41,680
world. So we realized that and that's

288
00:08:40,399 --> 00:08:43,360
that's that's what we're building

289
00:08:41,680 --> 00:08:46,000
clearly off and clearly is the new way

290
00:08:43,360 --> 00:08:47,440
you will use AI in um in 5 years

291
00:08:46,000 --> 00:08:50,000
hopefully if we do things right then in

292
00:08:47,440 --> 00:08:51,839
2 years. The phrase cheat on everything

293
00:08:50,000 --> 00:08:53,440
is intentionally ambiguous like what is

294
00:08:51,839 --> 00:08:54,560
cheat on like I know what cheat on test

295
00:08:53,440 --> 00:08:56,480
means but I don't know what cheat on

296
00:08:54,560 --> 00:08:58,240
everything means. It's left to be sort

297
00:08:56,480 --> 00:08:59,839
of confusing and make you sit on it and

298
00:08:58,240 --> 00:09:01,760
reflect for a moment. When you see

299
00:08:59,839 --> 00:09:03,120
someone using AI for everything, it

300
00:09:01,760 --> 00:09:04,160
makes you think this is unfair. They're

301
00:09:03,120 --> 00:09:06,399
not supposed to be doing that. They're

302
00:09:04,160 --> 00:09:08,160
cheating. In reality, like if you can

303
00:09:06,399 --> 00:09:09,680
use this for everything, like what does

304
00:09:08,160 --> 00:09:10,880
cheating on a meeting look like? It's

305
00:09:09,680 --> 00:09:12,480
not really a thing. It's just our gut

306
00:09:10,880 --> 00:09:14,000
human reaction to think this is so

307
00:09:12,480 --> 00:09:16,240
different. This is such a big advantage

308
00:09:14,000 --> 00:09:17,600
that it's unfair. And what we hope to do

309
00:09:16,240 --> 00:09:19,519
is we hope to give everyone this

310
00:09:17,600 --> 00:09:21,839
advantage. When every single person is

311
00:09:19,519 --> 00:09:23,440
using AI to cheat on meetings, then it's

312
00:09:21,839 --> 00:09:25,040
not that you're using you're cheating

313
00:09:23,440 --> 00:09:26,640
anymore. You're just this is just how

314
00:09:25,040 --> 00:09:28,959
humans will operate and think in the

315
00:09:26,640 --> 00:09:31,360
future. I think when you can use AI, you

316
00:09:28,959 --> 00:09:33,360
should use AI. If it helps you, then you

317
00:09:31,360 --> 00:09:34,399
should use it. If if using a calculator

318
00:09:33,360 --> 00:09:35,680
will help you, then you should use it.

319
00:09:34,399 --> 00:09:37,440
If using spell check will help you, then

320
00:09:35,680 --> 00:09:38,640
you will use it. Eventually, the spell

321
00:09:37,440 --> 00:09:40,240
check will teach you how to spell the

322
00:09:38,640 --> 00:09:42,560
right words cuz you'll get used to it so

323
00:09:40,240 --> 00:09:43,839
much. Or you just won't need to know how

324
00:09:42,560 --> 00:09:46,160
to spell anymore. You'll just need to

325
00:09:43,839 --> 00:09:48,160
know what the word is. If you can use AI

326
00:09:46,160 --> 00:09:49,519
to help, then you should. And if it can

327
00:09:48,160 --> 00:09:50,959
already do the job, then you'll never

328
00:09:49,519 --> 00:09:53,399
need to do the job in the future,

329
00:09:50,959 --> 00:09:56,080
assuming AI is everywhere, which it will

330
00:09:53,399 --> 00:09:57,760
be. All of technical interviews need to

331
00:09:56,080 --> 00:09:59,360
change. Not just in software engineering

332
00:09:57,760 --> 00:10:01,440
but everywhere in the world. If you get

333
00:09:59,360 --> 00:10:03,279
asked a question and AI can answer the

334
00:10:01,440 --> 00:10:04,800
question then you should probably get

335
00:10:03,279 --> 00:10:06,800
that out of your interview. I think

336
00:10:04,800 --> 00:10:08,640
interviews will be a lot more holistic

337
00:10:06,800 --> 00:10:10,080
and I really question whether we even

338
00:10:08,640 --> 00:10:11,600
need job interviews at all in the

339
00:10:10,080 --> 00:10:12,880
future. If there is an AI that knows

340
00:10:11,600 --> 00:10:14,720
everything about you, everything you're

341
00:10:12,880 --> 00:10:16,480
good at. Why do you need a 1-hour

342
00:10:14,720 --> 00:10:18,320
interview to assess anything other than

343
00:10:16,480 --> 00:10:19,839
culture fit? I already know all the work

344
00:10:18,320 --> 00:10:21,120
you've done or at least the AI already

345
00:10:19,839 --> 00:10:22,399
knows the work you've done. It knows how

346
00:10:21,120 --> 00:10:23,839
good it is. It knows what skills you're

347
00:10:22,399 --> 00:10:25,200
good at. Um, and if there is a skill

348
00:10:23,839 --> 00:10:27,360
match, then AI should just be able to

349
00:10:25,200 --> 00:10:29,360
match you directly to the job, assuming

350
00:10:27,360 --> 00:10:30,880
that we get along after like a 30-minute

351
00:10:29,360 --> 00:10:32,320
conversation. I really don't know that

352
00:10:30,880 --> 00:10:34,240
there is a need for interviews in

353
00:10:32,320 --> 00:10:35,680
today's age. But, but right now, what we

354
00:10:34,240 --> 00:10:37,279
use is uh it's really just a

355
00:10:35,680 --> 00:10:38,880
conversation. Uh, we check if you're a

356
00:10:37,279 --> 00:10:40,399
culture fit. We talk about past work

357
00:10:38,880 --> 00:10:42,160
you've done and um that's pretty much

358
00:10:40,399 --> 00:10:44,560
it. The whole point of Clo is to get

359
00:10:42,160 --> 00:10:46,560
everybody used to or used to a life

360
00:10:44,560 --> 00:10:48,480
where they use AI for everything. Once

361
00:10:46,560 --> 00:10:49,760
everybody uses AI in every instance

362
00:10:48,480 --> 00:10:51,360
possible, there's going to be a lot of

363
00:10:49,760 --> 00:10:53,519
jobs that get replaced and there's going

364
00:10:51,360 --> 00:10:55,440
to be a lot of people who are able to do

365
00:10:53,519 --> 00:10:57,519
so much more than they previously were.

366
00:10:55,440 --> 00:10:59,200
If every scientist decided one day, like

367
00:10:57,519 --> 00:11:01,120
today, I'm going to start using AI as

368
00:10:59,200 --> 00:11:02,720
much as possible. They will be 100 times

369
00:11:01,120 --> 00:11:04,399
more productive. When scientists are 100

370
00:11:02,720 --> 00:11:06,079
times more productive, we cure cancer 10

371
00:11:04,399 --> 00:11:07,680
years earlier. We cure Alzheimer's 10

372
00:11:06,079 --> 00:11:09,040
years earlier. Everyone lives to 400

373
00:11:07,680 --> 00:11:11,279
years old, and we're on the next flight

374
00:11:09,040 --> 00:11:13,839
to to Mars in like 2 years. the

375
00:11:11,279 --> 00:11:16,240
rate of societal progression will just

376
00:11:13,839 --> 00:11:18,160
expand and exponentiate significantly

377
00:11:16,240 --> 00:11:20,399
once everyone gets gets along to the

378
00:11:18,160 --> 00:11:21,920
fact that um we're all using AI now and

379
00:11:20,399 --> 00:11:23,600
and and that's what clearly hopes to

380
00:11:21,920 --> 00:11:26,560
achieve is to get everybody used to

381
00:11:23,600 --> 00:11:26,560
we're all using AI

382
00:11:27,640 --> 00:11:32,640
now. I think the the user experience I I

383
00:11:31,279 --> 00:11:34,800
spent a lot of time making the user

384
00:11:32,640 --> 00:11:36,399
experience very seamless. It's less of a

385
00:11:34,800 --> 00:11:38,959
technical challenge I think and more of

386
00:11:36,399 --> 00:11:41,040
a taste challenge. uh the the concept of

387
00:11:38,959 --> 00:11:42,320
an a translucent screen overlay,

388
00:11:41,040 --> 00:11:44,000
something that really has never been

389
00:11:42,320 --> 00:11:46,640
attempted before and it's something that

390
00:11:44,000 --> 00:11:48,800
that I tried and I think I only got to

391
00:11:46,640 --> 00:11:50,800
it after like dozens of iterations of

392
00:11:48,800 --> 00:11:53,120
different tools that would be a more

393
00:11:50,800 --> 00:11:54,480
seamless use of AI in your life. Yeah, I

394
00:11:53,120 --> 00:11:56,000
think that was the pro probably the

395
00:11:54,480 --> 00:11:57,760
biggest technical challenge was just

396
00:11:56,000 --> 00:11:59,360
figuring out like what exactly is the

397
00:11:57,760 --> 00:12:01,120
best user experience for someone using

398
00:11:59,360 --> 00:12:03,519
this tool. I mean I mean latency,

399
00:12:01,120 --> 00:12:05,519
response speed and um accuracy are like

400
00:12:03,519 --> 00:12:07,440
the the two biggest things. This is what

401
00:12:05,519 --> 00:12:09,200
every model, every like open AAI is

402
00:12:07,440 --> 00:12:11,200
working to improve latency and and

403
00:12:09,200 --> 00:12:13,279
accuracy. There there's ways that we can

404
00:12:11,200 --> 00:12:15,200
get to a much faster response. For

405
00:12:13,279 --> 00:12:17,200
example, if we host models on our own

406
00:12:15,200 --> 00:12:18,880
servers, uh this eliminates a lot of the

407
00:12:17,200 --> 00:12:20,480
latency that comes from the the load

408
00:12:18,880 --> 00:12:22,240
balancing and request uh request

409
00:12:20,480 --> 00:12:24,639
handling that that is just inherent in

410
00:12:22,240 --> 00:12:26,240
OpenAI servers. That's probably what we

411
00:12:24,639 --> 00:12:28,639
will end up doing. There's ways that we

412
00:12:26,240 --> 00:12:30,639
can cache the input and um sort of like

413
00:12:28,639 --> 00:12:33,360
uh parameterize the input so that um

414
00:12:30,639 --> 00:12:35,040
they you get the same condensed same

415
00:12:33,360 --> 00:12:37,360
same information but just condensed in a

416
00:12:35,040 --> 00:12:39,360
smaller way and the smaller the input

417
00:12:37,360 --> 00:12:41,680
size the faster the time to first token.

418
00:12:39,360 --> 00:12:43,839
Also generally accuracy can be improved

419
00:12:41,680 --> 00:12:46,320
by specific system prompts. We're

420
00:12:43,839 --> 00:12:47,600
developing custom evals in house uh

421
00:12:46,320 --> 00:12:49,200
based on a lot of the analytics and

422
00:12:47,600 --> 00:12:50,560
usage that we're seeing and like

423
00:12:49,200 --> 00:12:52,399
everything is getting better like every

424
00:12:50,560 --> 00:12:54,320
single day the model of the clue gets

425
00:12:52,399 --> 00:12:56,240
more accurate and faster. At a certain

426
00:12:54,320 --> 00:12:58,880
point, we're going to know uh exactly

427
00:12:56,240 --> 00:13:00,560
what type of responses you prefer as an

428
00:12:58,880 --> 00:13:02,959
individual. Um what sort of

429
00:13:00,560 --> 00:13:05,279
conversations that you're in and we can

430
00:13:02,959 --> 00:13:07,200
use all that data to generate a very

431
00:13:05,279 --> 00:13:09,200
very hyperspecific personalized

432
00:13:07,200 --> 00:13:11,120
fine-tuned model for you that knows that

433
00:13:09,200 --> 00:13:12,320
hey I'm a media reporter. I conduct

434
00:13:11,120 --> 00:13:14,240
these sorts of interviews and I

435
00:13:12,320 --> 00:13:16,240
generally want these types of responses.

436
00:13:14,240 --> 00:13:17,839
The tonality of my emails is this. So I

437
00:13:16,240 --> 00:13:18,959
would like you to respond in this way.

438
00:13:17,839 --> 00:13:20,720
And we can just get the most

439
00:13:18,959 --> 00:13:23,120
personalized model in the world. And um

440
00:13:20,720 --> 00:13:24,800
once we have that data as like a a moat

441
00:13:23,120 --> 00:13:25,920
defending us from the other big tech

442
00:13:24,800 --> 00:13:28,160
companies, then we'll pretty much be

443
00:13:25,920 --> 00:13:30,079
unstoppable. More so than the data, I

444
00:13:28,160 --> 00:13:32,079
think the the user experience is just

445
00:13:30,079 --> 00:13:33,440
interesting, untapped, and novel. Uh if

446
00:13:32,079 --> 00:13:35,360
we're correct about this, then we'll be

447
00:13:33,440 --> 00:13:37,200
the first to market. And there's a huge

448
00:13:35,360 --> 00:13:40,000
first mover advantage when you're when

449
00:13:37,200 --> 00:13:41,680
you're trying a new form of UX. And as

450
00:13:40,000 --> 00:13:43,120
if we can capture the market quickly

451
00:13:41,680 --> 00:13:45,519
enough by going viral enough

452
00:13:43,120 --> 00:13:47,120
sufficiently, then uh I think there will

453
00:13:45,519 --> 00:13:49,760
be it will be very hard to compete with

454
00:13:47,120 --> 00:13:52,000
us.

455
00:13:49,760 --> 00:13:53,600
the entire way we're going to think will

456
00:13:52,000 --> 00:13:55,279
be changed. Every single one of my

457
00:13:53,600 --> 00:13:57,360
thoughts is formulated by the

458
00:13:55,279 --> 00:13:59,040
information I have uh at this moment.

459
00:13:57,360 --> 00:14:00,880
But what happens when that information I

460
00:13:59,040 --> 00:14:02,800
have isn't just what's in my brain, but

461
00:14:00,880 --> 00:14:04,560
it's everything that humanity has ever

462
00:14:02,800 --> 00:14:06,720
collected and put online ever. What

463
00:14:04,560 --> 00:14:08,639
happens when AI literally helps me think

464
00:14:06,720 --> 00:14:10,079
in real time? The entire way that humans

465
00:14:08,639 --> 00:14:11,199
will interact with each other, with the

466
00:14:10,079 --> 00:14:12,560
world, all of our thoughts will be

467
00:14:11,199 --> 00:14:13,920
changed. like like what what happens

468
00:14:12,560 --> 00:14:16,000
when I know about every single post

469
00:14:13,920 --> 00:14:18,320
you've made online ever and I I I use

470
00:14:16,000 --> 00:14:21,199
that to distill down into like a like a

471
00:14:18,320 --> 00:14:23,120
condensed blurb of everything about you

472
00:14:21,199 --> 00:14:25,680
ever what does our interaction look like

473
00:14:23,120 --> 00:14:27,040
then it's it's really hard to say but I

474
00:14:25,680 --> 00:14:28,959
think this is a turning point for

475
00:14:27,040 --> 00:14:30,560
humanity and it it will fundamentally

476
00:14:28,959 --> 00:14:32,160
change the way that we think and the way

477
00:14:30,560 --> 00:14:33,680
that we behave as humans well if you're

478
00:14:32,160 --> 00:14:34,880
not building a company in AI right now

479
00:14:33,680 --> 00:14:38,160
then you're probably not doing the right

480
00:14:34,880 --> 00:14:40,000
thing just enables you to build such

481
00:14:38,160 --> 00:14:42,240
cool stuff and it's such a new

482
00:14:40,000 --> 00:14:43,279
technology that even If you're 19 and

483
00:14:42,240 --> 00:14:45,680
you've been playing with it for 2

484
00:14:43,279 --> 00:14:47,440
months, you are one of the the brightest

485
00:14:45,680 --> 00:14:49,199
minds. You are one of the pioneers of

486
00:14:47,440 --> 00:14:50,720
the field. It's not like biology where

487
00:14:49,199 --> 00:14:52,240
if you haven't studied for 10, 20 years,

488
00:14:50,720 --> 00:14:54,000
then you don't then you're not an expert

489
00:14:52,240 --> 00:14:55,680
in biology. You can study AI for 2

490
00:14:54,000 --> 00:14:58,160
months and you'll be an expert in AI.

491
00:14:55,680 --> 00:15:00,160
This technology is so gigantic and it's

492
00:14:58,160 --> 00:15:01,920
so new that you can be really really

493
00:15:00,160 --> 00:15:03,680
young and you can you can know it more

494
00:15:01,920 --> 00:15:05,279
deeply than anyone else and you you'll

495
00:15:03,680 --> 00:15:07,360
have the opportunity to build like a

496
00:15:05,279 --> 00:15:09,440
billion10 billion company out of it. I

497
00:15:07,360 --> 00:15:11,360
would say take bigger risks. This is the

498
00:15:09,440 --> 00:15:12,959
only advice I have for anyone really.

499
00:15:11,360 --> 00:15:14,399
You are smart enough, you're capable

500
00:15:12,959 --> 00:15:16,079
enough, you're hardworking enough, just

501
00:15:14,399 --> 00:15:17,760
take bigger risks. If you take bigger

502
00:15:16,079 --> 00:15:19,199
risks and force yourself into positions

503
00:15:17,760 --> 00:15:20,720
where you have to make it, you'll find

504
00:15:19,199 --> 00:15:22,240
that you're a lot more hardworking than

505
00:15:20,720 --> 00:15:23,519
you thought you were. And you'll also

506
00:15:22,240 --> 00:15:25,760
find that life gets a lot more

507
00:15:23,519 --> 00:15:27,279
interesting and very often the downside

508
00:15:25,760 --> 00:15:28,880
of risk is much smaller than you think

509
00:15:27,279 --> 00:15:30,639
and the upside of risk is much bigger

510
00:15:28,880 --> 00:15:32,560
than you think. I wasn't really like

511
00:15:30,639 --> 00:15:34,160
this 5 months ago. I mean, literally

512
00:15:32,560 --> 00:15:35,839
like like half a year ago, I was

513
00:15:34,160 --> 00:15:37,440
thinking I just want to get a job at a

514
00:15:35,839 --> 00:15:39,199
big tech company. That's all I want to

515
00:15:37,440 --> 00:15:40,639
do. And it wasn't until very recently

516
00:15:39,199 --> 00:15:42,079
that I thought like, oh, I actually want

517
00:15:40,639 --> 00:15:44,800
to build companies and go all in on

518
00:15:42,079 --> 00:15:47,519
this. Taking risks initially, it means

519
00:15:44,800 --> 00:15:49,120
being willing to uh get rid of the

520
00:15:47,519 --> 00:15:50,320
constructs and limiting beliefs in your

521
00:15:49,120 --> 00:15:51,680
mind that make you think, hey, when I

522
00:15:50,320 --> 00:15:53,199
graduate, I have to be an engineer,

523
00:15:51,680 --> 00:15:54,800
lawyer, a doctor, or whatever. Just

524
00:15:53,199 --> 00:15:57,040
being willing to see what would happen

525
00:15:54,800 --> 00:15:58,639
if you didn't do it. Every risk starts

526
00:15:57,040 --> 00:16:00,000
very small. Right now, I think people

527
00:15:58,639 --> 00:16:01,360
look at me and think I'm a crazy risk

528
00:16:00,000 --> 00:16:02,639
taker. But it didn't start like this.

529
00:16:01,360 --> 00:16:04,079
Started with me taking smaller risks.

530
00:16:02,639 --> 00:16:05,920
Like the first risk I took was, hey,

531
00:16:04,079 --> 00:16:07,360
what if I what if I built this tool and

532
00:16:05,920 --> 00:16:08,959
told nobody about it? Then the next risk

533
00:16:07,360 --> 00:16:10,480
I took was what if I posted online but

534
00:16:08,959 --> 00:16:11,839
made it free and didn't really associate

535
00:16:10,480 --> 00:16:13,120
with it so that people more people would

536
00:16:11,839 --> 00:16:14,720
see it. And eventually the risk just

537
00:16:13,120 --> 00:16:17,120
snowballed and snowballed until now I'm

538
00:16:14,720 --> 00:16:18,720
like fully posting whatever confidential

539
00:16:17,120 --> 00:16:20,480
document Columbia gives me because like

540
00:16:18,720 --> 00:16:21,920
I don't care like the risk is not it's

541
00:16:20,480 --> 00:16:23,519
not that much of a risk anymore and I've

542
00:16:21,920 --> 00:16:25,839
grown used to it. So grow used to taking

543
00:16:23,519 --> 00:16:25,839
bigger

544
00:16:26,600 --> 00:16:33,120
risks. I feel like my life is very easy.

545
00:16:30,959 --> 00:16:35,920
My life has been very easy. I mean my

546
00:16:33,120 --> 00:16:37,440
mom I've got loving parents. my mom, she

547
00:16:35,920 --> 00:16:38,720
made me study. Uh even when I didn't

548
00:16:37,440 --> 00:16:40,240
want to, she would just say, "Well, you

549
00:16:38,720 --> 00:16:41,759
should go study." So, I studied and and

550
00:16:40,240 --> 00:16:43,360
as a result, I did well in school and I

551
00:16:41,759 --> 00:16:44,880
hung out with smart kids and uh they

552
00:16:43,360 --> 00:16:46,800
they helped me do better in life. Like,

553
00:16:44,880 --> 00:16:48,959
I have two amazing great parents and I

554
00:16:46,800 --> 00:16:50,399
come from a great family. I don't really

555
00:16:48,959 --> 00:16:51,839
feel like my life has been all that

556
00:16:50,399 --> 00:16:53,759
challenging. Getting kicked out of

557
00:16:51,839 --> 00:16:54,959
Colombia, it's not that challenging when

558
00:16:53,759 --> 00:16:56,480
you're out there building companies and

559
00:16:54,959 --> 00:16:58,560
I was going to drop out anyways. Getting

560
00:16:56,480 --> 00:16:59,759
rescended from Harvard is also not that

561
00:16:58,560 --> 00:17:01,440
challenging when you have a loving

562
00:16:59,759 --> 00:17:03,279
family at home. There's kids like out

563
00:17:01,440 --> 00:17:04,880
there who are starving in like Uganda

564
00:17:03,279 --> 00:17:06,720
and like my life is not that hard

565
00:17:04,880 --> 00:17:08,400
really. A lot of everything that you

566
00:17:06,720 --> 00:17:10,240
should think is you should just try and

567
00:17:08,400 --> 00:17:12,079
think more positively about life and be

568
00:17:10,240 --> 00:17:14,000
more optimistic about things. It's very

569
00:17:12,079 --> 00:17:15,520
rare that you're going to be in America.

570
00:17:14,000 --> 00:17:17,280
You're going to have the opportunity to

571
00:17:15,520 --> 00:17:19,520
go to college and you're really in like

572
00:17:17,280 --> 00:17:21,039
an actually challenging situation. In

573
00:17:19,520 --> 00:17:22,720
reality, we're in the most interesting

574
00:17:21,039 --> 00:17:24,319
time in history. If you live in America

575
00:17:22,720 --> 00:17:26,000
and you're not like in poverty and your

576
00:17:24,319 --> 00:17:28,319
parents aren't crackheads, you have the

577
00:17:26,000 --> 00:17:30,080
the opportunity to make billions of

578
00:17:28,319 --> 00:17:31,919
dollars and and make generational wealth

579
00:17:30,080 --> 00:17:33,760
and do the most interesting thing ever.

580
00:17:31,919 --> 00:17:35,280
There's very few challenging situations

581
00:17:33,760 --> 00:17:37,760
that are so challenging that you're just

582
00:17:35,280 --> 00:17:39,520
like limited. Right now, anybody can do

583
00:17:37,760 --> 00:17:42,000
anything and you should just try and

584
00:17:39,520 --> 00:17:43,600
take risks and be bold because uh you're

585
00:17:42,000 --> 00:17:45,080
very privileged right now to be living

586
00:17:43,600 --> 00:17:48,320
in this

587
00:17:45,080 --> 00:17:51,200
world. Success is having a wife, having

588
00:17:48,320 --> 00:17:54,160
12 kids, and having people remember me.

589
00:17:51,200 --> 00:17:56,960
I think Steve Jobs and Elon Musk are

590
00:17:54,160 --> 00:17:58,799
very cool in that everyone has a strong

591
00:17:56,960 --> 00:18:00,480
opinion about them. Whether it's good or

592
00:17:58,799 --> 00:18:02,320
bad, everyone has something to say about

593
00:18:00,480 --> 00:18:04,160
Elon Musk and I think that's really

594
00:18:02,320 --> 00:18:05,679
cool. We're all going to die eventually

595
00:18:04,160 --> 00:18:07,280
and nobody's going to remember us in a

596
00:18:05,679 --> 00:18:09,600
thousand years. I might as well be

597
00:18:07,280 --> 00:18:11,440
remembered as strongly as possible for

598
00:18:09,600 --> 00:18:13,760
for the time that I'm here. I think the

599
00:18:11,440 --> 00:18:15,440
biggest thing is confidence. Like truly,

600
00:18:13,760 --> 00:18:16,880
you hear all the time that um the people

601
00:18:15,440 --> 00:18:18,720
that build big companies are not

602
00:18:16,880 --> 00:18:20,559
geniuses. They're not smarter than you.

603
00:18:18,720 --> 00:18:22,799
They're just um they take more risks

604
00:18:20,559 --> 00:18:24,160
than you and they're hard work harder

605
00:18:22,799 --> 00:18:25,919
workers than you. And I think this is

606
00:18:24,160 --> 00:18:27,360
generally true. 5 months ago, I was just

607
00:18:25,919 --> 00:18:28,960
some random student at some random

608
00:18:27,360 --> 00:18:31,440
school and I didn't really have anything

609
00:18:28,960 --> 00:18:33,280
going for me. And now I just raised $5

610
00:18:31,440 --> 00:18:34,720
million and I'm in this giant office and

611
00:18:33,280 --> 00:18:36,240
I'm building a company that I hope will

612
00:18:34,720 --> 00:18:38,000
change the world one day. And very

613
00:18:36,240 --> 00:18:39,919
little has changed about me except the

614
00:18:38,000 --> 00:18:41,600
fact that I took a risk. Even moving

615
00:18:39,919 --> 00:18:43,600
forward, if I do end up becoming like

616
00:18:41,600 --> 00:18:44,960
the next trillionaire, like as as as big

617
00:18:43,600 --> 00:18:46,640
as Mark Zuckerberg, there will be

618
00:18:44,960 --> 00:18:48,880
nothing about me that changed. It'll

619
00:18:46,640 --> 00:18:50,480
just be a series of well-calulated risks

620
00:18:48,880 --> 00:18:52,240
that I took that will lead me there. And

621
00:18:50,480 --> 00:18:53,840
I think the gap between Mark Zuckerberg

622
00:18:52,240 --> 00:18:55,600
and your average human, it's it's really

623
00:18:53,840 --> 00:18:57,520
not that big. And if you just have the

624
00:18:55,600 --> 00:19:01,400
confidence to take bigger risks, then

625
00:18:57,520 --> 00:19:01,400
very often you will win.

